General Information about Version 1: (Due Wednesday)

I will give feedback on the version submissions to help cater to your specific later version submissions. As for the criteria for grading version 1, I am looking for 5 items corresponding to a good setup for the rest of the semester:

    Github Repository Formatting: The system should either be cookiecutter formatting as described in the syllabus, or something better (for those experienced with GIT). Better just means higher expected usability and lower deployment risk, which becomes very apparent early on in the GIT troubleshooting forums.
    Documentation/Organization: The syllabus covers what should be in the ReadMe such as citations and collaborations. Files should be in reasonable directories. Dependencies such as Requirements.txt, Environment.yml, CMakeList.txt, package.json, vcpkg.json, conanfile.txt, etc are needed depending on your preferred language/package manager.
    A sufficient representation of an MDP: Policies and subtasks are a given, but why rewards and states? Here is the justification: despite the environment already being fully implemented, you will need your agent to eventually bootstrap its own understanding of the reward function and stochastic transitions (also called model-based RL or world-model learning). You will eventually need to give agents a way to store and update prior beliefs of what the foundation environment is as an MDP.
    Dynamic Programming (DP) implementation: Specifically, Policy iteration and Policy improvement on both values and Q-values. For environments without sufficiently simple subtasks, you can trade this requirement out for backward view TD(lambda) + greedy improvements on expected values. You can then implement DP once we get to function approximation later.
    Framework for Agent implementation: Given a suitable subtask, a list of arguments (hyperparameters), and an agent model, the program will either further train or evaluate the agent in your foundation environment. Also have a copy of your best policy kernel + value function pair if you've trained it at least once. 

While not strictly necessary, it will be important to have the following things done sooner rather than later so as not to get behind on new content for V2:

Value Iteration

Monte Carlo methods

forward view TD(n) via bootstrapped returns/values + greedy improvements on expected values.

backward view TD(lambda) via Eligibility Traces + greedy improvements on expected values.

forward view Sarsa(n) via bootstrapped returns/Q-values + greedy improvements on Q-values

backward view Sarsa(lambda) via Eligibility Traces + greedy improvements on Q-values

Exploration injection

Q-learning


It also is very useful early on to have two utilities files. One for an explicit raster_mode=human for seeing how the agent is doing, as well as generic visualization/plot tools for your own purposes. Note that cookiecutter specifically has a directory for Jupyter notebooks, which are great for experimentation. 

For the next 4 weeks, I highly recommend looking into tools like Tensorboard and Wandb. Both automatically keep track and plot specified variables and parameters over times steps. These are extremely helpful when making comparisons between different agents empirically. Wandb in particular is amazing for hyperparameter optimization (free for academic use).